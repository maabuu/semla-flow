{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate conditionally generated molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly import express as px\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = Path(\"/homes/buttensc/Projects/semla-flow/evaluation/generated/conditional\")\n",
    "files = list(pred_dir.glob(\"*_conditional.csv\"))\n",
    "dfs = []\n",
    "for file in tqdm(files):\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"method\"] = \" \".join(file.stem.split(\"_\")[1:3])\n",
    "    df = df.dropna(subset=[\"Reference molecule\"])\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs).sort_values([\"method\", \"Reference molecule\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/homes/buttensc/Projects/semla-flow/evaluation/truth/test_first_1000.csv\"\n",
    "truth = pd.read_csv(file)\n",
    "truth = truth[truth.fail != 1.0]\n",
    "print(len(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"sucos\": \"SuCOS\",\n",
    "    \"tanimoto\": \"ECFP4 Bit Tanimoto\",\n",
    "    \"ensemble_avg_energy\": \"Ensemble Average Energy\",\n",
    "    \"mol_pred_energy\": \"Molecular Prediction Energy\",\n",
    "    \"energy_ratio\": \"Energy Ratio\",\n",
    "    \"sa\": \"Synthetic Accessability Score\",\n",
    "    \"sa_normalized\": \"Synthetic Accessability Score (normalized)\",\n",
    "    \"spacial\": \"Spacial Score\",\n",
    "    \"qed\": \"QED\",\n",
    "    \"logp\": \"LogP\",\n",
    "    \"lipinski\": \"Lipinski Rule of 5\",\n",
    "    \"num_heavy\": \"Number of Heavy Atoms\",\n",
    "    \"weight\": \"Molecular Weight\",\n",
    "    \"num_rings\": \"Number of Rings\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"sucos\"\n",
    "# metric = \"tanimoto\"\n",
    "name = metrics[metric]\n",
    "sns.histplot(\n",
    "    df[[\"method\", metric]].reset_index(drop=True),\n",
    "    x=metric,\n",
    "    hue=\"method\",\n",
    "    bins=100,\n",
    "    # cumulative=True,\n",
    "    common_norm=False,\n",
    "    stat=\"density\",\n",
    "    element=\"step\",\n",
    "    # kde=True,\n",
    "    fill=False,\n",
    "    # legend=True, palette=\"tab10\", linewidth=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = \"sucos\"\n",
    "metric = \"tanimoto\"\n",
    "name = metrics[metric]\n",
    "sns.histplot(\n",
    "    df[[\"method\", metric]].reset_index(drop=True),\n",
    "    x=metric,\n",
    "    hue=\"method\",\n",
    "    bins=100,\n",
    "    cumulative=True,\n",
    "    common_norm=False,\n",
    "    stat=\"density\",\n",
    "    element=\"step\",\n",
    "    # kde=True,\n",
    "    fill=False,\n",
    "    # legend=True, palette=\"tab10\", linewidth=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novelty and uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv(\"evaluation/truth/train.csv\")\n",
    "# df_train[\"method\"] = \"GEOM Drugs Training\"\n",
    "\n",
    "df_test = pd.read_csv(\"evaluation/truth/test.csv\")\n",
    "df_test[\"method\"] = \"GEOM Drugs Testing\"\n",
    "\n",
    "testing_smiles = set(df_test[\"smiles\"].dropna()) - {None, \"\", pd.NA, np.nan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_uniquenss(smiles: list[str]) -> float:\n",
    "    \"\"\"Compute the uniqueness of a list of SMILES strings.\"\"\"\n",
    "    valid_smiles = [s for s in smiles if s not in {None, \"\", pd.NA, np.nan}]  # list\n",
    "    return len(set(valid_smiles)) / len(valid_smiles)\n",
    "\n",
    "\n",
    "def compute_novelty(\n",
    "    smiles: list[str], reference_smiles: set[str] = testing_smiles\n",
    ") -> float:\n",
    "    \"\"\"How many are not in the test set?\"\"\"\n",
    "    # valid_smiles = set(s for s in smiles if s not in {None, \"\", pd.NA, np.nan})  # set\n",
    "    valid_smiles = list(s for s in smiles if s not in {None, \"\", pd.NA, np.nan})  # list\n",
    "    return len(\n",
    "        [smiles for smiles in valid_smiles if smiles not in reference_smiles]\n",
    "    ) / len(valid_smiles)\n",
    "\n",
    "\n",
    "def compute_unique_novelty(\n",
    "    smiles: list[str], reference_smiles: set[str] = testing_smiles\n",
    ") -> float:\n",
    "    \"\"\"How many unique new molecules have we generated?\"\"\"\n",
    "    # valid_smiles = set(s for s in smiles if s not in {None, \"\", pd.NA, np.nan})  # set\n",
    "    valid_smiles = list(s for s in smiles if s not in {None, \"\", pd.NA, np.nan})  # list\n",
    "    return len(set(valid_smiles) - reference_smiles) / len(valid_smiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much repetition is there? How unique are the generated molecules?\n",
    "df.groupby(\"method\")[\"smiles_pred\"].agg(compute_uniqueness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the valid generated molecules are not in the test set?\n",
    "df.groupby(\"method\")[\"smiles_pred\"].agg(compute_novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the valid generated molecules are in the test set?\n",
    "(df.groupby(\"method\")[\"smiles_pred\"].agg(compute_novelty) - 1).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many valid, unique and new molecules have we generated?\n",
    "df.groupby(\"method\")[\"smiles_pred\"].agg(compute_unique_novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
